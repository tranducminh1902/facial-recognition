{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Train_model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPDcX6I4k2Zy6sH49qkwax5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SznRi-BbjL4V","executionInfo":{"status":"ok","timestamp":1632667337632,"user_tz":-420,"elapsed":67856,"user":{"displayName":"Long Nguyen Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ09nXjU-7AnI9OnAWffCEIvC4ypg0U-POB1pVAg=s64","userId":"04250606013751201459"}},"outputId":"72004887-d7a6-4e8d-db51-1513b802ce7d"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}]},{"cell_type":"code","metadata":{"id":"QW7omr_xjXRQ","executionInfo":{"status":"ok","timestamp":1632667341909,"user_tz":-420,"elapsed":2654,"user":{"displayName":"Long Nguyen Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ09nXjU-7AnI9OnAWffCEIvC4ypg0U-POB1pVAg=s64","userId":"04250606013751201459"}}},"source":["import tensorflow as tf\n","import pathlib\n","from tensorflow import keras\n","import glob\n","from keras_preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, BatchNormalization, Dropout\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"dglfcpJjjfzm","executionInfo":{"status":"ok","timestamp":1632667539938,"user_tz":-420,"elapsed":4941,"user":{"displayName":"Long Nguyen Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ09nXjU-7AnI9OnAWffCEIvC4ypg0U-POB1pVAg=s64","userId":"04250606013751201459"}}},"source":["!unzip -q '/content/gdrive/MyDrive/Facial_Classification/images_260921.zip' -d /content/Data/"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"S6CMUHjBj21x","executionInfo":{"status":"ok","timestamp":1632667599931,"user_tz":-420,"elapsed":347,"user":{"displayName":"Long Nguyen Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ09nXjU-7AnI9OnAWffCEIvC4ypg0U-POB1pVAg=s64","userId":"04250606013751201459"}}},"source":["data_dir = pathlib.Path('/content/Data')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A0ycXcogn5H7","executionInfo":{"status":"ok","timestamp":1632668468923,"user_tz":-420,"elapsed":323,"user":{"displayName":"Long Nguyen Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ09nXjU-7AnI9OnAWffCEIvC4ypg0U-POB1pVAg=s64","userId":"04250606013751201459"}},"outputId":"2044cb70-0c26-4d7f-bb1f-c0ff5cb35648"},"source":["train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","                                                                data_dir,\n","                                                                validation_split = 0.2,\n","                                                                subset = 'training',\n","                                                                seed = 123,\n","                                                                image_size = (224,224),\n","                                                                batch_size = 32\n","                                                              )"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1773 files belonging to 3 classes.\n","Using 1419 files for training.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ip0Di9Ktn5xE","executionInfo":{"status":"ok","timestamp":1632668471011,"user_tz":-420,"elapsed":306,"user":{"displayName":"Long Nguyen Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ09nXjU-7AnI9OnAWffCEIvC4ypg0U-POB1pVAg=s64","userId":"04250606013751201459"}},"outputId":"94146bb6-c0b4-4e0a-a2ec-05c35acefd4d"},"source":["val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","                                                            data_dir,\n","                                                            validation_split = 0.2,\n","                                                            subset = 'validation',\n","                                                            seed = 123,\n","                                                            image_size = (224,224),\n","                                                            batch_size = 32\n","                                                            )"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1773 files belonging to 3 classes.\n","Using 354 files for validation.\n"]}]},{"cell_type":"code","metadata":{"id":"H8lZkQ-PmGlb","executionInfo":{"status":"ok","timestamp":1632668328844,"user_tz":-420,"elapsed":345,"user":{"displayName":"Long Nguyen Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ09nXjU-7AnI9OnAWffCEIvC4ypg0U-POB1pVAg=s64","userId":"04250606013751201459"}}},"source":["IMG_SIZE = 224"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"ujd4gcszkvb6","executionInfo":{"status":"ok","timestamp":1632668330880,"user_tz":-420,"elapsed":858,"user":{"displayName":"Long Nguyen Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ09nXjU-7AnI9OnAWffCEIvC4ypg0U-POB1pVAg=s64","userId":"04250606013751201459"}}},"source":["base_model = keras.applications.MobileNetV2(weights='imagenet',\n","                                              input_shape=(IMG_SIZE, IMG_SIZE, 3),\n","                                              include_top=False)\n","\n","base_model.trainable = False"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZzFJXrzSofVy","executionInfo":{"status":"ok","timestamp":1632668622020,"user_tz":-420,"elapsed":365,"user":{"displayName":"Long Nguyen Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ09nXjU-7AnI9OnAWffCEIvC4ypg0U-POB1pVAg=s64","userId":"04250606013751201459"}}},"source":["from keras.layers import  GlobalAveragePooling2D"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"a8n1lV7Gk3re","executionInfo":{"status":"ok","timestamp":1632668623459,"user_tz":-420,"elapsed":7,"user":{"displayName":"Long Nguyen Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ09nXjU-7AnI9OnAWffCEIvC4ypg0U-POB1pVAg=s64","userId":"04250606013751201459"}}},"source":["from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","\n","def model_maker():\n","  inputs = keras.Input(shape=(224,224,3))\n","  x = preprocess_input(inputs)\n","  x = base_model(x, training=False)\n","  x = GlobalAveragePooling2D()(x)\n","  x = Dense(128, activation = 'relu')(x)\n","  x = Dropout(0.25)(x)\n","  outputs = Dense(3, activation='softmax')(x)\n","  model = keras.Model(inputs, outputs)\n","  return model"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nWtB5ICSlE_m","executionInfo":{"status":"ok","timestamp":1632668625467,"user_tz":-420,"elapsed":511,"user":{"displayName":"Long Nguyen Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ09nXjU-7AnI9OnAWffCEIvC4ypg0U-POB1pVAg=s64","userId":"04250606013751201459"}},"outputId":"a3af548c-7252-4ff7-d308-5e917a7413d9"},"source":["model = model_maker()\n","model.summary()"],"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_10 (InputLayer)        [(None, 224, 224, 3)]     0         \n","_________________________________________________________________\n","tf.math.truediv_7 (TFOpLambd (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","tf.math.subtract_7 (TFOpLamb (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","mobilenetv2_1.00_224 (Functi (None, 7, 7, 1280)        2257984   \n","_________________________________________________________________\n","global_average_pooling2d (Gl (None, 1280)              0         \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 128)               163968    \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 3)                 387       \n","=================================================================\n","Total params: 2,422,339\n","Trainable params: 164,355\n","Non-trainable params: 2,257,984\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"5OZnFjyglAv4","executionInfo":{"status":"ok","timestamp":1632668630907,"user_tz":-420,"elapsed":324,"user":{"displayName":"Long Nguyen Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ09nXjU-7AnI9OnAWffCEIvC4ypg0U-POB1pVAg=s64","userId":"04250606013751201459"}}},"source":["model.compile(optimizer='adam',\n","               loss = 'sparse_categorical_crossentropy',\n","               metrics = ['accuracy'])"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"cbChVlW_lRrG","executionInfo":{"status":"ok","timestamp":1632668632672,"user_tz":-420,"elapsed":350,"user":{"displayName":"Long Nguyen Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ09nXjU-7AnI9OnAWffCEIvC4ypg0U-POB1pVAg=s64","userId":"04250606013751201459"}}},"source":["from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","callback = [EarlyStopping(monitor = 'val_loss', \n","                          patience=10, \n","                          restore_best_weights=True),\n","\n","            ReduceLROnPlateau(monitor = 'val_loss',\n","                              patience = 10,\n","                              factor = 0.7,\n","                              min_lr = 1e-2),\n","\n","            ModelCheckpoint(filepath=\"/content/gdrive/MyDrive/Facial_Classification/model_1.h5\",\n","                                                 save_weights_only=False, # the whole model (False) or only weights (True) \n","                                                 save_best_only=True, # keep the best model with lowest validation loss\n","                                                 monitor='val_loss',\n","                                                 verbose=1 )]\n"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rmxDH3iflcbX","executionInfo":{"status":"ok","timestamp":1632669370030,"user_tz":-420,"elapsed":735412,"user":{"displayName":"Long Nguyen Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ09nXjU-7AnI9OnAWffCEIvC4ypg0U-POB1pVAg=s64","userId":"04250606013751201459"}},"outputId":"21bd141c-1d6e-483e-c079-4a6638473598"},"source":["history = model.fit(train_ds,\n","                              validation_data = val_ds,\n","                              epochs = 10,\n","                              callbacks = callback)"],"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","45/45 [==============================] - 48s 1s/step - loss: 0.1247 - accuracy: 0.9535 - val_loss: 0.0368 - val_accuracy: 0.9859\n","\n","Epoch 00001: val_loss improved from inf to 0.03677, saving model to /content/gdrive/MyDrive/Facial_Classification/model_1.h5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/10\n","45/45 [==============================] - 45s 995ms/step - loss: 0.0132 - accuracy: 0.9965 - val_loss: 0.0059 - val_accuracy: 0.9972\n","\n","Epoch 00002: val_loss improved from 0.03677 to 0.00590, saving model to /content/gdrive/MyDrive/Facial_Classification/model_1.h5\n","Epoch 3/10\n","45/45 [==============================] - 45s 992ms/step - loss: 0.0117 - accuracy: 0.9965 - val_loss: 0.0083 - val_accuracy: 0.9972\n","\n","Epoch 00003: val_loss did not improve from 0.00590\n","Epoch 4/10\n","45/45 [==============================] - 45s 987ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.0064 - val_accuracy: 0.9972\n","\n","Epoch 00004: val_loss did not improve from 0.00590\n","Epoch 5/10\n","45/45 [==============================] - 45s 991ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 0.9972\n","\n","Epoch 00005: val_loss improved from 0.00590 to 0.00390, saving model to /content/gdrive/MyDrive/Facial_Classification/model_1.h5\n","Epoch 6/10\n","45/45 [==============================] - 45s 987ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 0.9972\n","\n","Epoch 00006: val_loss improved from 0.00390 to 0.00378, saving model to /content/gdrive/MyDrive/Facial_Classification/model_1.h5\n","Epoch 7/10\n","45/45 [==============================] - 43s 955ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n","\n","Epoch 00007: val_loss improved from 0.00378 to 0.00246, saving model to /content/gdrive/MyDrive/Facial_Classification/model_1.h5\n","Epoch 8/10\n","45/45 [==============================] - 43s 957ms/step - loss: 8.4753e-04 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n","\n","Epoch 00008: val_loss did not improve from 0.00246\n","Epoch 9/10\n","45/45 [==============================] - 43s 953ms/step - loss: 6.6338e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n","\n","Epoch 00009: val_loss improved from 0.00246 to 0.00184, saving model to /content/gdrive/MyDrive/Facial_Classification/model_1.h5\n","Epoch 10/10\n","45/45 [==============================] - 43s 955ms/step - loss: 9.9006e-04 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n","\n","Epoch 00010: val_loss did not improve from 0.00184\n"]}]}]}